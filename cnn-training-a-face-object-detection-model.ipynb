{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"The aim of this is to be able to accurately detect human faces (if any) in a picture and to localise the face with a fitting bounding box.\n\nWith awarenesss of already trained models that can perform such task in just a few lines of code, such as the face detetcion model from OpenCV, I will be training a new model instead using the images dataset attached to this notebook. I am treating this as a standard object detection task rather than specifically focusing on facial detection.\n\nThe dataset contains images of people, dimensions of the images, and the bounding box coordinates of people's faces present in each picture.\n\nLets get started.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"# defining a few configurations\n\nimport torch\n\nBATCH_SIZE = 4 # increase / decrease according to GPU memeory\nRESIZE_TO = 512 # resize the image for training and transforms\n\nDEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\nCLASSES = ['background', 'Face']\nNUM_CLASSES = 2","metadata":{"execution":{"iopub.status.busy":"2023-06-08T18:35:57.626165Z","iopub.execute_input":"2023-06-08T18:35:57.626624Z","iopub.status.idle":"2023-06-08T18:35:57.634157Z","shell.execute_reply.started":"2023-06-08T18:35:57.62659Z","shell.execute_reply":"2023-06-08T18:35:57.633129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check if cuda (GPU) is available\n\ntorch.cuda.is_available()","metadata":{"execution":{"iopub.status.busy":"2023-06-08T18:35:57.657936Z","iopub.execute_input":"2023-06-08T18:35:57.660502Z","iopub.status.idle":"2023-06-08T18:35:57.67203Z","shell.execute_reply.started":"2023-06-08T18:35:57.660468Z","shell.execute_reply":"2023-06-08T18:35:57.669812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split dataset into train, validation, and train sets\n\nimport pandas as pd\nimport shutil\nimport os\n\ndef train_valid_test_split(faces_csv=None, split=0.15):\n    all_df = pd.read_csv(faces_csv)     \n    \n    # sample out 500 images\n    all_df = all_df.sample(n=500, random_state=7)\n    \n    # Shuffle the CSV file rows.\n    all_df.sample(frac=1)\n    len_df = len(all_df)\n    \n    # Split into train/validation and test sets\n    trainTest_split = int((1-split)*len_df)\n    \n    trainVal_df = all_df[:trainTest_split]\n    test_df = all_df[trainTest_split:]\n    \n    # Further split train/validation set into train and validation sets\n    lenTV_df = len(trainVal_df)\n    \n    trainVal_split = int((1-split)*lenTV_df)\n    \n    train_df = trainVal_df[:trainVal_split]\n    valid_df = trainVal_df[trainVal_split:]\n    \n    return train_df, valid_df, test_df\n    \ntrain_df, valid_df, test_df = train_valid_test_split(faces_csv='/kaggle/input/human-faces-object-detection/faces.csv')","metadata":{"execution":{"iopub.status.busy":"2023-07-02T10:57:37.258601Z","iopub.execute_input":"2023-07-02T10:57:37.259068Z","iopub.status.idle":"2023-07-02T10:57:37.303380Z","shell.execute_reply.started":"2023-07-02T10:57:37.259041Z","shell.execute_reply":"2023-07-02T10:57:37.302548Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"Above, we split the dataset into training, validation, and test sets.\n\nBut before that, we sample out only 500 entries in the dataset (of about 3000 total). This is surely small to train a whole CNN model on, as the 3000+ itself. However, because training an efficient CNN model from scratch requires a whole lot resources and tens of thousands to millions of images, we will be using pretrained model that has been trained on the COCO dataset - a benchmark dataset for most object detection tasks. \n\nWe will perform transfer learning by fine tuning the model to fit our task so that the model can now accurately identify and localise objects in our custom dataset (that is not in the COCO dataset). This way, we can have a functional model from a dataset as small as ours.","metadata":{}},{"cell_type":"code","source":"import cv2\nimport numpy as np\n\n# this class keeps track of the training and validation loss values...\n# ... and helps to get the average for each epoch as well\n\nclass Averager:\n    \"\"\"\"\"\n    this class keeps track of the training and validation loss values...\n    and helps to get the average for each epoch as well\n    \"\"\"\"\"\n    \n    def __init__(self):\n        self.current_total = 0.0\n        self.iterations = 0.0\n        \n    def send(self, value):\n        self.current_total += value\n        self.iterations += 1\n    \n    @property\n    def value(self):\n        if self.iterations == 0:\n            return 0\n        else:\n            return 1.0 * self.current_total / self.iterations\n    \n    def reset(self):\n        self.current_total = 0.0\n        self.iterations = 0.0\n        \n        \ndef collate_fn(batch):\n    \"\"\"\n    To handle the data loading as different images may have different number \n    of objects and to handle varying size tensors as well.\n    \"\"\"\n    return tuple(zip(*batch))","metadata":{"execution":{"iopub.status.busy":"2023-06-08T18:35:57.737425Z","iopub.execute_input":"2023-06-08T18:35:57.739678Z","iopub.status.idle":"2023-06-08T18:35:57.975872Z","shell.execute_reply.started":"2023-06-08T18:35:57.739646Z","shell.execute_reply":"2023-06-08T18:35:57.974916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import glob as glob\nfrom torch.utils.data import Dataset, DataLoader\n\n# Creating the dataset class\n\nclass Faces(Dataset):\n    def __init__(self, dataset, width, height, dir_path=\"/kaggle/input/human-faces-object-detection/images\"):\n        self.dir_path = dir_path\n        self.height = height\n        self.width = width\n        self.dataset = dataset\n        \n        # copy image names to list\n        self.set_image_names = self.dataset['image_name'].tolist()\n        \n        # get all the image names in sorted order\n        self.image_paths = glob.glob(f\"{self.dir_path}/*.jpg\")\n        self.all_images = [image_path.split('/')[-1] for image_path in self.image_paths]\n        self.all_images = sorted(self.all_images)\n        \n        # cut down to only images present in dataset\n        self.images = []\n        \n        for i in self.set_image_names:\n            for j in self.all_images:\n                if i == j:\n                    self.images.append(i)\n        \n    def __getitem__(self, idx):\n        \n        # capture the image name and the full image path\n        image_name = self.images[idx]\n        image_path = os.path.join(self.dir_path, image_name)\n        \n        # read the image\n        image = cv2.imread(image_path)\n        \n        # convert BGR to RGB color format and resize\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image_resized = cv2.resize(image, (self.width, self.height))\n        image_resized /= 255.0\n        \n        # channel first transposing\n        image_resized = np.transpose(image_resized, (2, 0, 1))\n             \n        boxes = []\n        labels = []\n\n        # Copy bounding box coordinates and image dimensions\n        filtered_df = self.dataset.loc[self.dataset['image_name'] == image_name]\n        \n        for i in range(len(filtered_df)):\n\n            # xmax = left corner x-coordinates\n            xmin = int(filtered_df['x0'].iloc[i])\n            # xmax = right corner x-coordinates\n            xmax = int(filtered_df['x1'].iloc[i])\n            # ymin = left corner y-coordinates\n            ymin = int(filtered_df['y0'].iloc[i])\n            # ymax = right corner y-coordinates\n            ymax = int(filtered_df['y1'].iloc[i])\n\n            image_width = int(filtered_df['width'].iloc[i])\n            image_height = int(filtered_df['height'].iloc[i])\n\n            # resize the bounding boxes according to the...\n            # ... desired `width`, `height`\n            xmin_final = (xmin/image_width)*self.width\n            xmax_final = (xmax/image_width)*self.width\n            ymin_final = (ymin/image_height)*self.height\n            yamx_final = (ymax/image_height)*self.height\n\n            boxes.append([xmin_final, ymin_final, xmax_final, yamx_final])\n            labels.append(1) # 1 because there is only one class\n        \n        # bounding box to tensor\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        \n        # area of the bounding boxes\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        \n        # crowd instances\n        if boxes.shape[0] > 1:\n            iscrowd = torch.ones((boxes.shape[0],), dtype=torch.int64)\n        else:\n            iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n            \n        # label to tensor\n        labels = torch.as_tensor(labels, dtype=torch.int64)\n\n        # prepare the final `target` dictionary\n        target = {}\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n        target[\"area\"] = area\n        target[\"iscrowd\"] = iscrowd\n        image_id = torch.tensor([idx])\n        target[\"image_id\"] = image_id\n\n        return image_resized, target\n    \n    def __len__(self):\n        return len(self.set_image_names)","metadata":{"execution":{"iopub.status.busy":"2023-06-08T18:35:57.980569Z","iopub.execute_input":"2023-06-08T18:35:57.982855Z","iopub.status.idle":"2023-06-08T18:35:58.008224Z","shell.execute_reply.started":"2023-06-08T18:35:57.982821Z","shell.execute_reply":"2023-06-08T18:35:58.007116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The code above is a class that iterates over each image in either of our train or validation set, performs a few operations, and returns the image and target (bounding box coordinates, object label, e.t.c.) components.\n\nBelow we will pass in the train and validation sets and store the returned components.\n\nWe also defined our data loaders below. These loaders will help load data in bacthes into the model during training and validation.","metadata":{}},{"cell_type":"code","source":"train_dataset = Faces(train_df, RESIZE_TO, RESIZE_TO)\nvalid_dataset = Faces(valid_df, RESIZE_TO, RESIZE_TO)\n\n# defining train and validation sets data loaders\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    num_workers=0,\n    collate_fn=collate_fn\n)\nvalid_loader = DataLoader(\n    valid_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=0,\n    collate_fn=collate_fn\n)\n\nprint(f\"Number of training samples: {len(train_dataset)}\")\nprint(f\"Number of validation samples: {len(valid_dataset)}\\n\")","metadata":{"execution":{"iopub.status.busy":"2023-06-08T18:35:58.01455Z","iopub.execute_input":"2023-06-08T18:35:58.017174Z","iopub.status.idle":"2023-06-08T18:35:59.468675Z","shell.execute_reply.started":"2023-06-08T18:35:58.017141Z","shell.execute_reply":"2023-06-08T18:35:59.467225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"At this point, our image and target components are ready, and we will preview a number of them below to confirm everything is as expected before moving on to traning a model on them.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# function to visualize sample\n\ndef visualize_sample(image, target):\n    for box in target['boxes']:\n        cv2.rectangle(\n            image, \n            (int(box[0]), int(box[1])), (int(box[2]), int(box[3])),\n            (0, 255, 0), 1\n        )\n    \n    plt.imshow(image)\n    plt.axis('off')\n    plt.show()\n\nNUM_SAMPLES_TO_VISUALIZE = 5\nfor i in range(NUM_SAMPLES_TO_VISUALIZE):\n    image, target = train_dataset[i]\n    image = np.transpose(image, (1, 2, 0))\n    visualize_sample(image, target)","metadata":{"execution":{"iopub.status.busy":"2023-06-08T18:35:59.473017Z","iopub.execute_input":"2023-06-08T18:35:59.475456Z","iopub.status.idle":"2023-06-08T18:36:01.148719Z","shell.execute_reply.started":"2023-06-08T18:35:59.475419Z","shell.execute_reply":"2023-06-08T18:36:01.14777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Above is what the images in the dataset look like with the bounding boxes.\n\nNormally, there are bounding box coordinates for each face (where there are more than one person) in almost all the images in the dataset. The reason some faces don't have here is because we have sampled only 500 from the entire dataset, as such, some bounding box coordinates wouldn't have made the sample set. This doesn't mean the dataset is bad though, it is just a subset of the entire dataset and still very much efficient to train with, as we will see later on.\n\n**Defining the model**\n\nThe model that we will be using is the pretrained Faster RCNN with a Resnet 50 backbone. We will fine tune the box head predictor so the output matches the number of classes in our custom dataset, which is 2. The first class is reserved for the background and the the second class is our faces.","metadata":{}},{"cell_type":"code","source":"# defining model\n\nimport torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\ndef create_model(num_classes):\n    \n    # load Faster RCNN pre-trained model\n    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights='DEFAULT')\n    \n    # get the number of input features \n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    # define a new head for the detector with required number of classes\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) \n    return model","metadata":{"execution":{"iopub.status.busy":"2023-06-08T18:36:01.150439Z","iopub.execute_input":"2023-06-08T18:36:01.155095Z","iopub.status.idle":"2023-06-08T18:36:01.461423Z","shell.execute_reply.started":"2023-06-08T18:36:01.155045Z","shell.execute_reply":"2023-06-08T18:36:01.460381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we will define functions for running training and validation iterations then begin the training process.","metadata":{}},{"cell_type":"code","source":"# function for running training iterations\n\ndef train(train_data_loader, model):\n    print('Training')\n    global train_itr\n    global train_loss_list\n    \n     # initialize tqdm progress bar\n    prog_bar = tqdm(train_data_loader, total=len(train_data_loader))\n    \n    for i, data in enumerate(prog_bar):\n        optimizer.zero_grad()\n        images, targets = data\n        \n        images = list(torch.from_numpy(image).to(DEVICE) for image in images)\n        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n        loss_dict = model(images, targets)\n        losses = sum(loss for loss in loss_dict.values())\n        loss_value = losses.item()\n        train_loss_list.append(loss_value)\n        train_loss_hist.send(loss_value)\n        losses.backward()\n        optimizer.step()\n        train_itr += 1\n    \n        # update the loss value beside the progress bar for each iteration\n        prog_bar.set_description(desc=f\"Loss: {loss_value:.4f}\")\n    return train_loss_list","metadata":{"execution":{"iopub.status.busy":"2023-06-08T18:36:01.46614Z","iopub.execute_input":"2023-06-08T18:36:01.468528Z","iopub.status.idle":"2023-06-08T18:36:01.480281Z","shell.execute_reply.started":"2023-06-08T18:36:01.468493Z","shell.execute_reply":"2023-06-08T18:36:01.479213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function for running validation iterations\n\ndef validate(valid_data_loader, model):\n    print('Validating')\n    global val_itr\n    global val_loss_list\n    \n    # initialize tqdm progress bar\n    prog_bar = tqdm(valid_data_loader, total=len(valid_data_loader))\n    \n    for i, data in enumerate(prog_bar):\n        images, targets = data\n        \n        images = list(torch.from_numpy(image).to(DEVICE) for image in images)\n        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n        \n        with torch.no_grad():\n            loss_dict = model(images, targets)\n        losses = sum(loss for loss in loss_dict.values())\n        loss_value = losses.item()\n        val_loss_list.append(loss_value)\n        val_loss_hist.send(loss_value)\n        val_itr += 1\n        # update the loss value beside the progress bar for each iteration\n        prog_bar.set_description(desc=f\"Loss: {loss_value:.4f}\")\n    return val_loss_list","metadata":{"execution":{"iopub.status.busy":"2023-06-08T18:36:01.482925Z","iopub.execute_input":"2023-06-08T18:36:01.483263Z","iopub.status.idle":"2023-06-08T18:36:01.497136Z","shell.execute_reply.started":"2023-06-08T18:36:01.483231Z","shell.execute_reply":"2023-06-08T18:36:01.494875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will be training our model for only 10 epochs.\n\nAfter a number of tests, I have found out that the optimal number of epochs to use for this task is 10. The model tends to start overfitting at higher epochs. One may say this is due to the small dataset, however, even with a larger or augmented set, it tends to start overfitting at higher epochs.\n\nA meaningful explanation as to why is that the model had at 10 epochs learned all it needed to know to be able to correctly perform the object detection task, such that more epochs on the data is no longer beneficiary.\n\nAs we will see during evaluation on the test set, the model after training for 10 epochs performs brilliantly.","metadata":{}},{"cell_type":"code","source":"# training the model\n\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\nimport time\n\nplt.style.use('ggplot')\n\nNUM_EPOCHS = 10 # number of epochs to train for\n\n# initialize the model and move to the computation device\nmodel = create_model(num_classes=NUM_CLASSES)\nmodel = model.to(DEVICE)\n# get the model parameters\nparams = [p for p in model.parameters() if p.requires_grad]\n# define the optimizer\noptimizer = torch.optim.SGD(params, lr=0.001, momentum=0.9, weight_decay=0.0005)\n# initialize the Averager class\ntrain_loss_hist = Averager()\nval_loss_hist = Averager()\ntrain_itr = 1\nval_itr = 1\n# train and validation loss lists to store loss values of all...\n# ... iterations till ena and plot graphs for all iterations\ntrain_loss_list = []\nval_loss_list = []\n# name to save the trained model with\nMODEL_NAME = 'model'\n\n# start the training epochs\nfor epoch in range(NUM_EPOCHS):\n    print(f\"\\nEPOCH {epoch+1} of {NUM_EPOCHS}\")\n    \n    # reset the training and validation loss histories for the current epoch\n    train_loss_hist.reset()\n    val_loss_hist.reset()\n    \n    # start timer and carry out training and validation\n    start = time.time()\n    train_loss = train(train_loader, model)\n    val_loss = validate(valid_loader, model)\n    \n    print(f\"Epoch #{epoch+1} train loss: {train_loss_hist.value:.3f}\")   \n    print(f\"Epoch #{epoch+1} validation loss: {val_loss_hist.value:.3f}\")   \n    end = time.time()\n    print(f\"Took {((end - start) / 60):.3f} minutes for epoch {epoch+1}\")\n\n    if (epoch+1) == NUM_EPOCHS: # save loss plots and model once at the end\n        # create two subplots, one for each, training and validation\n        figure_1, train_ax = plt.subplots()\n        figure_2, valid_ax = plt.subplots()        \n        train_ax.plot(train_loss, color='blue')\n        train_ax.set_xlabel('iterations')\n        train_ax.set_ylabel('train loss')\n        valid_ax.plot(val_loss, color='red')\n        valid_ax.set_xlabel('iterations')\n        valid_ax.set_ylabel('validation loss')\n        figure_1.savefig(f\"/kaggle/working/train_loss_{epoch+1}.png\")\n        figure_2.savefig(f\"/kaggle/working/valid_loss_{epoch+1}.png\")\n        torch.save(model.state_dict(), f\"/kaggle/working/model{epoch+1}.pth\")\n\ntrain_ax.plot(train_loss, color='blue')\ntrain_ax.set_xlabel('iterations')\ntrain_ax.set_ylabel('train loss')\n\nvalid_ax.plot(val_loss, color='red')\nvalid_ax.set_xlabel('iterations')\nvalid_ax.set_ylabel('validation loss')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-08T18:36:01.502189Z","iopub.execute_input":"2023-06-08T18:36:01.503169Z","iopub.status.idle":"2023-06-08T18:48:35.026665Z","shell.execute_reply.started":"2023-06-08T18:36:01.503124Z","shell.execute_reply":"2023-06-08T18:48:35.025756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Testing trained model on 5 random images from the test set**","metadata":{}},{"cell_type":"code","source":"# testing trained model on test set\n\nimport random\nmodel.eval()\n\n# define the detection threshold...\n# ... any detection having score below this will be discarded\ndetection_threshold = 0.8\n\n# copy image names to list\ntest_image_names = test_df['image_name'].tolist()\n\n# get all the image names in sorted order\ndir_path=\"/kaggle/input/human-faces-object-detection/images\"\nimage_paths = glob.glob(f\"{dir_path}/*.jpg\")\nall_images = [image_path.split('/')[-1] for image_path in image_paths]\nall_images = sorted(all_images)\n\n# get paths of only images present in dataset\ntest_images = []\n\nfor i in test_image_names:\n    for j in all_images:\n        if i == j:\n            test_images.append(os.path.join(dir_path, i))\n            \n\nprint(f\"Test instances: {len(test_images)}\")\n\nfor i in random.sample(range(len(test_images)), 5):\n    image = cv2.imread(test_images[i])\n    orig_image = image.copy()\n    # BGR to RGB\n    image = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB).astype(np.float32)\n    # make the pixel range between 0 and 1\n    image /= 255.0\n    # bring color channels to front\n    image = np.transpose(image, (2, 0, 1)).astype(float)\n    # convert to tensor\n    image = torch.tensor(image, dtype=torch.float).cuda()\n    # add batch dimension\n    image = torch.unsqueeze(image, 0)\n    with torch.no_grad():\n        outputs = model(image)\n    \n    # load all detection to CPU for further operations\n    outputs = [{k: v.to('cpu') for k, v in t.items()} for t in outputs]\n    # carry further only if there are detected boxes\n    if len(outputs[0]['boxes']) != 0:\n        boxes = outputs[0]['boxes'].data.numpy()\n        scores = outputs[0]['scores'].data.numpy()\n        # filter out boxes according to `detection_threshold`\n        boxes = boxes[scores >= detection_threshold].astype(np.int32)\n        draw_boxes = boxes.copy()\n        \n        # draw the bounding boxes\n        for j, box in enumerate(draw_boxes):\n            cv2.rectangle(orig_image,\n                        (int(box[0]), int(box[1])),\n                        (int(box[2]), int(box[3])),\n                        (0, 0, 255), 2)\n        image_rgb = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB)\n        plt.imshow(image_rgb)\n        plt.axis('off')\n        plt.show()\n    print(f\"Image {i+1} done...\")\n    print('-'*50)\nprint('TEST PREDICTIONS COMPLETE')","metadata":{"execution":{"iopub.status.busy":"2023-06-08T18:48:35.029843Z","iopub.execute_input":"2023-06-08T18:48:35.030489Z","iopub.status.idle":"2023-06-08T18:48:38.225764Z","shell.execute_reply.started":"2023-06-08T18:48:35.03045Z","shell.execute_reply":"2023-06-08T18:48:38.224524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, even though we trained with only 500 samples and for only 10 epochs, the model does brilliant on new unseen images of people.\n\nThis is only a simple demonstration though; an ideal application of the model may require us to train with more data to enhance the model's effectiveness in detecting even more intricate faces.","metadata":{}}]}